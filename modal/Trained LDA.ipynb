{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0da20f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afzal\\AppData\\Local\\Temp\\ipykernel_16400\\1903849182.py:8: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv('true.csv', error_bad_lines=False);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afzal\\AppData\\Local\\Temp\\ipykernel_16400\\1903849182.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index #creating a new cloumn index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>21412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>21413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>21414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>21415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>21416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  index\n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...  21412\n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...  21413\n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...  21414\n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...  21415\n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  21416"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy.linalg import norm\n",
    "from string import digits\n",
    "import pickle\n",
    "\n",
    "data = pd.read_csv('true.csv', error_bad_lines=False);\n",
    "data_text = data[['text']]\n",
    "data_text['index'] = data_text.index #creating a new cloumn index.\n",
    "documents = data_text\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "documents.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83cf2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\afzal\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\afzal\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\afzal\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\afzal\\anaconda3\\lib\\site-packages (from nltk) (2022.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\afzal\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\afzal\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9d9f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\afzal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37ff2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d37c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v')) # stemmer was not defined soo top code fixes it\n",
    "\n",
    "def preprocess (text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len (token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eddac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 20].values[0][0]\n",
    "# print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "# print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "# print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa609648",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeddd572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [washington, reuter, head, conserv, republican...\n",
       "1    [washington, reuter, transgend, peopl, allow, ...\n",
       "2    [washington, reuter, special, counsel, investi...\n",
       "3    [washington, reuter, trump, campaign, advis, g...\n",
       "4    [seattl, washington, reuter, presid, donald, t...\n",
       "5    [west, palm, beach, washington, reuter, white,...\n",
       "6    [west, palm, beach, reuter, presid, donald, tr...\n",
       "7    [follow, statement, post, verifi, twitter, acc...\n",
       "8    [follow, statement, post, verifi, twitter, acc...\n",
       "9    [washington, reuter, alabama, secretari, state...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5393222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 action\n",
      "1 administr\n",
      "2 agre\n",
      "3 aid\n",
      "4 approach\n",
      "5 approv\n",
      "6 arriv\n",
      "7 assist\n",
      "8 balloon\n",
      "9 battl\n",
      "10 begin\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62763626",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce96bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 1),\n",
       " (36, 2),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (66, 1),\n",
       " (68, 2),\n",
       " (70, 1),\n",
       " (129, 1),\n",
       " (204, 2),\n",
       " (212, 1),\n",
       " (221, 1),\n",
       " (250, 1),\n",
       " (265, 1),\n",
       " (324, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (359, 1),\n",
       " (434, 3),\n",
       " (474, 1),\n",
       " (482, 1),\n",
       " (499, 1),\n",
       " (530, 1),\n",
       " (540, 1),\n",
       " (577, 2),\n",
       " (598, 1),\n",
       " (620, 1),\n",
       " (656, 2),\n",
       " (703, 2),\n",
       " (716, 1),\n",
       " (743, 1),\n",
       " (752, 1),\n",
       " (767, 3),\n",
       " (794, 4),\n",
       " (839, 1),\n",
       " (868, 1),\n",
       " (895, 1),\n",
       " (967, 2),\n",
       " (1070, 1),\n",
       " (1124, 1),\n",
       " (1320, 1),\n",
       " (1384, 1),\n",
       " (1419, 1),\n",
       " (1425, 1),\n",
       " (1482, 1),\n",
       " (1541, 1),\n",
       " (1615, 1),\n",
       " (1744, 1),\n",
       " (1775, 2),\n",
       " (1825, 1),\n",
       " (1988, 1),\n",
       " (2050, 1),\n",
       " (2092, 1),\n",
       " (2109, 1),\n",
       " (2236, 1),\n",
       " (2263, 1),\n",
       " (2275, 4),\n",
       " (2276, 3),\n",
       " (2441, 1),\n",
       " (2653, 1),\n",
       " (2821, 1),\n",
       " (2947, 1),\n",
       " (3009, 1),\n",
       " (3054, 1),\n",
       " (3264, 2),\n",
       " (3268, 1),\n",
       " (3665, 1),\n",
       " (3813, 2),\n",
       " (4026, 1),\n",
       " (4205, 1),\n",
       " (4630, 1),\n",
       " (5236, 1),\n",
       " (5496, 2),\n",
       " (5682, 1),\n",
       " (5769, 1),\n",
       " (6147, 3),\n",
       " (6333, 1),\n",
       " (7755, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[21416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db8ca5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 36 (\"countri\") appears 1 time.\n",
      "Word 53 (\"donald\") appears 1 time.\n",
      "Word 86 (\"govern\") appears 1 time.\n",
      "Word 129 (\"militari\") appears 1 time.\n",
      "Word 131 (\"month\") appears 1 time.\n",
      "Word 144 (\"peopl\") appears 1 time.\n",
      "Word 170 (\"respons\") appears 1 time.\n",
      "Word 172 (\"return\") appears 1 time.\n",
      "Word 192 (\"speak\") appears 1 time.\n",
      "Word 198 (\"support\") appears 1 time.\n",
      "Word 206 (\"trump\") appears 1 time.\n",
      "Word 209 (\"urg\") appears 2 time.\n",
      "Word 213 (\"washington\") appears 1 time.\n",
      "Word 224 (\"access\") appears 1 time.\n",
      "Word 227 (\"allow\") appears 2 time.\n",
      "Word 254 (\"depart\") appears 3 time.\n",
      "Word 265 (\"forc\") appears 1 time.\n",
      "Word 300 (\"major\") appears 1 time.\n",
      "Word 334 (\"right\") appears 1 time.\n",
      "Word 339 (\"secretari\") appears 1 time.\n",
      "Word 340 (\"senior\") appears 1 time.\n",
      "Word 347 (\"statement\") appears 2 time.\n",
      "Word 407 (\"investig\") appears 1 time.\n",
      "Word 410 (\"launch\") appears 1 time.\n",
      "Word 429 (\"report\") appears 1 time.\n",
      "Word 450 (\"alleg\") appears 1 time.\n",
      "Word 482 (\"includ\") appears 1 time.\n",
      "Word 498 (\"respond\") appears 1 time.\n",
      "Word 525 (\"chief\") appears 2 time.\n",
      "Word 532 (\"concern\") appears 1 time.\n",
      "Word 548 (\"end\") appears 1 time.\n",
      "Word 575 (\"media\") appears 1 time.\n",
      "Word 712 (\"secur\") appears 1 time.\n",
      "Word 721 (\"tougher\") appears 1 time.\n",
      "Word 754 (\"general\") appears 1 time.\n",
      "Word 776 (\"pressur\") appears 1 time.\n",
      "Word 793 (\"thursday\") appears 1 time.\n",
      "Word 805 (\"express\") appears 1 time.\n",
      "Word 1006 (\"neighbor\") appears 1 time.\n",
      "Word 1031 (\"consid\") appears 1 time.\n",
      "Word 1047 (\"mount\") appears 1 time.\n",
      "Word 1075 (\"ahead\") appears 1 time.\n",
      "Word 1076 (\"attend\") appears 1 time.\n",
      "Word 1235 (\"muslim\") appears 3 time.\n",
      "Word 1265 (\"abus\") appears 1 time.\n",
      "Word 1302 (\"flee\") appears 2 time.\n",
      "Word 1355 (\"violenc\") appears 1 time.\n",
      "Word 1380 (\"safe\") appears 1 time.\n",
      "Word 1438 (\"crisi\") appears 1 time.\n",
      "Word 1455 (\"formal\") appears 1 time.\n",
      "Word 1466 (\"human\") appears 1 time.\n",
      "Word 1607 (\"area\") appears 1 time.\n",
      "Word 1615 (\"cooper\") appears 1 time.\n",
      "Word 1667 (\"visit\") appears 1 time.\n",
      "Word 1775 (\"southeast\") appears 1 time.\n",
      "Word 1925 (\"attack\") appears 1 time.\n",
      "Word 2263 (\"asian\") appears 1 time.\n",
      "Word 2272 (\"ethnic\") appears 2 time.\n",
      "Word 2915 (\"facilit\") appears 1 time.\n",
      "Word 2950 (\"declar\") appears 1 time.\n",
      "Word 3009 (\"asia\") appears 1 time.\n",
      "Word 3162 (\"milit\") appears 1 time.\n",
      "Word 3684 (\"tillerson\") appears 3 time.\n",
      "Word 3814 (\"armi\") appears 2 time.\n",
      "Word 4360 (\"humanitarian\") appears 1 time.\n",
      "Word 4553 (\"crackdown\") appears 2 time.\n",
      "Word 4607 (\"summit\") appears 1 time.\n",
      "Word 5107 (\"atroc\") appears 1 time.\n",
      "Word 5108 (\"aung\") appears 2 time.\n",
      "Word 5109 (\"bangladesh\") appears 1 time.\n",
      "Word 5111 (\"buddhist\") appears 1 time.\n",
      "Word 5114 (\"cleans\") appears 1 time.\n",
      "Word 5121 (\"manila\") appears 1 time.\n",
      "Word 5123 (\"myanmar\") appears 5 time.\n",
      "Word 5127 (\"rakhin\") appears 2 time.\n",
      "Word 5129 (\"rohingya\") appears 6 time.\n",
      "Word 5141 (\"displac\") appears 1 time.\n",
      "Word 5316 (\"maiden\") appears 1 time.\n",
      "Word 6330 (\"hla\") appears 2 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_997 = bow_corpus[997]\n",
    "\n",
    "for i in range(len(bow_doc_997)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_997[i][0], \n",
    "                                               dictionary[bow_doc_997[i][0]],\n",
    "                                                         bow_doc_997[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6504d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bbb06f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.026841685091095916),\n",
      " (1, 0.040874722914628694),\n",
      " (2, 0.02835365174741251),\n",
      " (3, 0.03660473880060063),\n",
      " (4, 0.039605125068353925),\n",
      " (5, 0.03092179071764382),\n",
      " (6, 0.03897867535244838),\n",
      " (7, 0.08024173683071609),\n",
      " (8, 0.0740943853428846),\n",
      " (9, 0.03788393586016424),\n",
      " (10, 0.027398418040369982),\n",
      " (11, 0.035840612711428195),\n",
      " (12, 0.04961255800869147),\n",
      " (13, 0.05782661322801332),\n",
      " (14, 0.03974153302302116),\n",
      " (15, 0.0291238513421231),\n",
      " (16, 0.0616129444077586),\n",
      " (17, 0.07687915789702442),\n",
      " (18, 0.02705966268183348),\n",
      " (19, 0.17385661598683122),\n",
      " (20, 0.041903357918051526),\n",
      " (21, 0.03124372753859764),\n",
      " (22, 0.01827310221513898),\n",
      " (23, 0.053251745351518935),\n",
      " (24, 0.031618017464307764),\n",
      " (25, 0.022935188852024425),\n",
      " (26, 0.06781583839128565),\n",
      " (27, 0.03762650186346645),\n",
      " (28, 0.051686343451263946),\n",
      " (29, 0.025333152711866598),\n",
      " (30, 0.017199206457189753),\n",
      " (31, 0.04758258500547958),\n",
      " (32, 0.06620182303451502),\n",
      " (33, 0.08780959236607397),\n",
      " (34, 0.04958184699935254),\n",
      " (35, 0.04224534531710516),\n",
      " (36, 0.013540996020924581),\n",
      " (37, 0.030187557046097286),\n",
      " (38, 0.11704206379170323),\n",
      " (39, 0.14415928113258095),\n",
      " (40, 0.040982105549432955),\n",
      " (41, 0.07588430904467848),\n",
      " (42, 0.13277683836915255),\n",
      " (43, 0.08016488300039695),\n",
      " (44, 0.03489170995578394),\n",
      " (45, 0.08968274716344637),\n",
      " (46, 0.0608533909710637),\n",
      " (47, 0.04821650941410047),\n",
      " (48, 0.09106036092846707),\n",
      " (49, 0.05037003927263967),\n",
      " (50, 0.06269870287511947),\n",
      " (51, 0.2523784093827648),\n",
      " (52, 0.026172721198332048),\n",
      " (53, 0.01102643099014463),\n",
      " (54, 0.03460122112256553),\n",
      " (55, 0.14359043107948868),\n",
      " (56, 0.0304813276585664),\n",
      " (57, 0.031780875394950045),\n",
      " (58, 0.04319766277805212),\n",
      " (59, 0.05844214207344302),\n",
      " (60, 0.014371060446726692),\n",
      " (61, 0.035483522894322564),\n",
      " (62, 0.12985467564616968),\n",
      " (63, 0.046406236861776515),\n",
      " (64, 0.04777446329370804),\n",
      " (65, 0.06443441416009102),\n",
      " (66, 0.03927615199637036),\n",
      " (67, 0.0539264683755197),\n",
      " (68, 0.02206875859505465),\n",
      " (69, 0.05187218145980983),\n",
      " (70, 0.023239475105942937),\n",
      " (71, 0.0536738637577313),\n",
      " (72, 0.03443327913253746),\n",
      " (73, 0.06895274761193664),\n",
      " (74, 0.02534154170929118),\n",
      " (75, 0.0725491516260888),\n",
      " (76, 0.1732458562711467),\n",
      " (77, 0.08019047614026772),\n",
      " (78, 0.02352895870708413),\n",
      " (79, 0.044632515457056855),\n",
      " (80, 0.040927455661766594),\n",
      " (81, 0.05380584826528975),\n",
      " (82, 0.03165854432134186),\n",
      " (83, 0.045260648362014856),\n",
      " (84, 0.06339888628041054),\n",
      " (85, 0.042080682415839864),\n",
      " (86, 0.02294918312015768),\n",
      " (87, 0.032022030234916773),\n",
      " (88, 0.042260416833262086),\n",
      " (89, 0.03522410817474467),\n",
      " (90, 0.025158201918787034),\n",
      " (91, 0.06955977200749831),\n",
      " (92, 0.029923882170165565),\n",
      " (93, 0.020071794561354044),\n",
      " (94, 0.03981653466446631),\n",
      " (95, 0.05671465698392012),\n",
      " (96, 0.09922381748782771),\n",
      " (97, 0.048335335496447314),\n",
      " (98, 0.053567064356381804),\n",
      " (99, 0.034416601634745546),\n",
      " (100, 0.09344294493625994),\n",
      " (101, 0.08085067441611211),\n",
      " (102, 0.05534825529609657),\n",
      " (103, 0.04470488364855402),\n",
      " (104, 0.04590023581869888),\n",
      " (105, 0.043861553924264336),\n",
      " (106, 0.03679289609330034),\n",
      " (107, 0.02829057410480336),\n",
      " (108, 0.07384517895151353),\n",
      " (109, 0.03420988823613656),\n",
      " (110, 0.05779199869162798),\n",
      " (111, 0.040159407308087695),\n",
      " (112, 0.03125219631895542),\n",
      " (113, 0.05227405096872812),\n",
      " (114, 0.0323993863135558),\n",
      " (115, 0.06129114619627013),\n",
      " (116, 0.01842642587475128),\n",
      " (117, 0.03182190517138409),\n",
      " (118, 0.071607082394652),\n",
      " (119, 0.027806606567218322),\n",
      " (120, 0.03244595287421068),\n",
      " (121, 0.07084087617568535),\n",
      " (122, 0.31005636805973313),\n",
      " (123, 0.02889682654458893),\n",
      " (124, 0.05731277039204787),\n",
      " (125, 0.06588170450371987),\n",
      " (126, 0.04093850801359174),\n",
      " (127, 0.036467789680463235),\n",
      " (128, 0.0463444845908447),\n",
      " (129, 0.023100616674346396),\n",
      " (130, 0.03302568233832187),\n",
      " (131, 0.015511661030319123),\n",
      " (132, 0.06372785276103247),\n",
      " (133, 0.03807139950925253),\n",
      " (134, 0.059778366000846135),\n",
      " (135, 0.09441147691466534),\n",
      " (136, 0.035456449693473356),\n",
      " (137, 0.04496133705253271),\n",
      " (138, 0.1340602633264679),\n",
      " (139, 0.15094341269035835),\n",
      " (140, 0.03512930150005831),\n",
      " (141, 0.0882696557445402),\n",
      " (142, 0.037353156199957466),\n",
      " (143, 0.07687382328364882),\n",
      " (144, 0.026416535359105285),\n",
      " (145, 0.04650043431323718),\n",
      " (146, 0.052842684268690425),\n",
      " (147, 0.06357583667359604),\n",
      " (148, 0.04275262480138838),\n",
      " (149, 0.0438105894912676),\n",
      " (150, 0.03482278339457484),\n",
      " (151, 0.04286201898252382),\n",
      " (152, 0.19331114975264269),\n",
      " (153, 0.06509581467465815),\n",
      " (154, 0.027989869370252718),\n",
      " (155, 0.05497389480952625),\n",
      " (156, 0.025951187475818174),\n",
      " (157, 0.02131428013876665),\n",
      " (158, 0.055928767154858394),\n",
      " (159, 0.0488712386243755),\n",
      " (160, 0.028005256520326344),\n",
      " (161, 0.08377344211962101),\n",
      " (162, 0.04890634290597571),\n",
      " (163, 0.0313377543690067),\n",
      " (164, 0.038448210449855864),\n",
      " (165, 0.08324136862768126),\n",
      " (166, 0.2109011608758769),\n",
      " (167, 0.029174212006305263),\n",
      " (168, 0.029636301526595615),\n",
      " (169, 0.042185230422708636),\n",
      " (170, 0.05193754850468235),\n",
      " (171, 0.07071500283365412),\n",
      " (172, 0.029682828413619952),\n",
      " (173, 0.04987813428204682),\n",
      " (174, 0.05834001605247908),\n",
      " (175, 0.05635959528308644),\n",
      " (176, 0.08668831879432323),\n",
      " (177, 0.0397290739980805),\n",
      " (178, 0.06740572013387652),\n",
      " (179, 0.052775720814254586),\n",
      " (180, 0.03975400379621904),\n",
      " (181, 0.026498393703619394),\n",
      " (182, 0.04452693906417771),\n",
      " (183, 0.04560567455559876),\n",
      " (184, 0.05788518829208938),\n",
      " (185, 0.03840307302038781),\n",
      " (186, 0.040791810841941306),\n",
      " (187, 0.05547052353826321),\n",
      " (188, 0.025785269270674916),\n",
      " (189, 0.06141892742216852),\n",
      " (190, 0.036198073768944135),\n",
      " (191, 0.06544328024109033),\n",
      " (192, 0.024123830693590222),\n",
      " (193, 0.0386074157739083),\n",
      " (194, 0.18123436659963552),\n",
      " (195, 0.06671726152487872),\n",
      " (196, 0.041830168721874963),\n",
      " (197, 0.0560413231874011),\n",
      " (198, 0.031163367434273805),\n",
      " (199, 0.05190340993712112),\n",
      " (200, 0.023412403800145344),\n",
      " (201, 0.010480637796350637),\n",
      " (202, 0.0413008017897576),\n",
      " (203, 0.023365345230731756),\n",
      " (204, 0.02745244886536704),\n",
      " (205, 0.15102667769886022),\n",
      " (206, 0.07405448102085016),\n",
      " (207, 0.03307829779730541),\n",
      " (208, 0.013248563012986849),\n",
      " (209, 0.030693435335108547),\n",
      " (210, 0.07995022795984635),\n",
      " (211, 0.07233899841664404),\n",
      " (212, 0.0775962507043078),\n",
      " (213, 0.023064127845226087),\n",
      " (214, 0.03551901160523123),\n",
      " (215, 0.04410201971238327),\n",
      " (216, 0.06526646548103152),\n",
      " (217, 0.019754506836286767),\n",
      " (218, 0.08377344211962101),\n",
      " (219, 0.04570313449551585),\n",
      " (220, 0.01742786453885472),\n",
      " (221, 0.0280063831108891),\n",
      " (222, 0.043100892195102804)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel (bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c80ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10,id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84c81269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.024*\"parti\" + 0.016*\"elect\" + 0.012*\"vote\" + 0.007*\"percent\" + 0.007*\"year\" + 0.007*\"poll\" + 0.007*\"court\" + 0.007*\"trump\" + 0.006*\"support\" + 0.006*\"rule\"\n",
      "Topic: 1 \n",
      "Words: 0.043*\"trump\" + 0.009*\"russian\" + 0.008*\"russia\" + 0.007*\"republican\" + 0.007*\"washington\" + 0.007*\"donald\" + 0.007*\"hous\" + 0.007*\"white\" + 0.006*\"offici\" + 0.006*\"putin\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"minist\" + 0.009*\"european\" + 0.008*\"govern\" + 0.006*\"britain\" + 0.006*\"deal\" + 0.006*\"trump\" + 0.006*\"countri\" + 0.006*\"year\" + 0.005*\"tell\" + 0.005*\"leader\"\n",
      "Topic: 3 \n",
      "Words: 0.008*\"year\" + 0.008*\"percent\" + 0.008*\"trump\" + 0.006*\"peopl\" + 0.006*\"govern\" + 0.006*\"hurrican\" + 0.006*\"million\" + 0.005*\"irma\" + 0.005*\"florida\" + 0.005*\"island\"\n",
      "Topic: 4 \n",
      "Words: 0.007*\"militari\" + 0.007*\"polic\" + 0.007*\"peopl\" + 0.007*\"myanmar\" + 0.007*\"attack\" + 0.007*\"year\" + 0.006*\"govern\" + 0.006*\"kill\" + 0.006*\"secur\" + 0.006*\"rohingya\"\n",
      "Topic: 5 \n",
      "Words: 0.026*\"north\" + 0.023*\"korea\" + 0.012*\"nuclear\" + 0.009*\"unit\" + 0.008*\"missil\" + 0.008*\"china\" + 0.008*\"sanction\" + 0.007*\"korean\" + 0.007*\"govern\" + 0.006*\"test\"\n",
      "Topic: 6 \n",
      "Words: 0.011*\"turkey\" + 0.011*\"china\" + 0.009*\"govern\" + 0.008*\"year\" + 0.007*\"countri\" + 0.006*\"unit\" + 0.006*\"turkish\" + 0.006*\"peopl\" + 0.006*\"court\" + 0.005*\"erdogan\"\n",
      "Topic: 7 \n",
      "Words: 0.009*\"court\" + 0.008*\"polic\" + 0.007*\"year\" + 0.007*\"peopl\" + 0.005*\"govern\" + 0.005*\"case\" + 0.005*\"citi\" + 0.004*\"rule\" + 0.004*\"feder\" + 0.004*\"health\"\n",
      "Topic: 8 \n",
      "Words: 0.021*\"trump\" + 0.020*\"republican\" + 0.014*\"senat\" + 0.013*\"democrat\" + 0.011*\"hous\" + 0.009*\"clinton\" + 0.009*\"campaign\" + 0.009*\"elect\" + 0.006*\"committe\" + 0.006*\"obama\"\n",
      "Topic: 9 \n",
      "Words: 0.009*\"govern\" + 0.009*\"syria\" + 0.008*\"iran\" + 0.008*\"syrian\" + 0.008*\"forc\" + 0.008*\"islam\" + 0.007*\"unit\" + 0.007*\"saudi\" + 0.006*\"group\" + 0.005*\"iraq\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00964455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.014*\"maduro\" + 0.012*\"venezuela\" + 0.007*\"venezuelan\" + 0.005*\"trump\" + 0.004*\"caraca\" + 0.004*\"nicola\" + 0.003*\"republican\" + 0.003*\"unesco\" + 0.003*\"senat\" + 0.003*\"scottish\"\n",
      "Topic: 1 Word: 0.010*\"macron\" + 0.007*\"cuba\" + 0.006*\"merkel\" + 0.005*\"kurz\" + 0.005*\"poland\" + 0.005*\"cuban\" + 0.005*\"european\" + 0.004*\"euro\" + 0.004*\"french\" + 0.004*\"mogadishu\"\n",
      "Topic: 2 Word: 0.007*\"merkel\" + 0.006*\"saudi\" + 0.006*\"pakistan\" + 0.004*\"coalit\" + 0.004*\"india\" + 0.004*\"qatar\" + 0.004*\"german\" + 0.004*\"germani\" + 0.003*\"hurrican\" + 0.003*\"arabia\"\n",
      "Topic: 3 Word: 0.004*\"refuge\" + 0.004*\"trump\" + 0.004*\"asylum\" + 0.003*\"republican\" + 0.003*\"australia\" + 0.003*\"senat\" + 0.003*\"seeker\" + 0.003*\"hous\" + 0.002*\"immigr\" + 0.002*\"polic\"\n",
      "Topic: 4 Word: 0.012*\"israel\" + 0.012*\"palestinian\" + 0.011*\"jerusalem\" + 0.010*\"ireland\" + 0.008*\"isra\" + 0.006*\"irish\" + 0.005*\"britain\" + 0.005*\"brexit\" + 0.005*\"netanyahu\" + 0.004*\"northern\"\n",
      "Topic: 5 Word: 0.012*\"temer\" + 0.005*\"brazil\" + 0.005*\"babi\" + 0.005*\"czech\" + 0.004*\"trump\" + 0.004*\"brazilian\" + 0.004*\"brasilia\" + 0.003*\"senat\" + 0.003*\"hous\" + 0.003*\"pragu\"\n",
      "Topic: 6 Word: 0.016*\"hariri\" + 0.006*\"nairobi\" + 0.004*\"trump\" + 0.003*\"ratcliff\" + 0.003*\"senat\" + 0.003*\"republican\" + 0.003*\"clinton\" + 0.003*\"sinai\" + 0.003*\"elect\" + 0.002*\"hous\"\n",
      "Topic: 7 Word: 0.007*\"spain\" + 0.007*\"court\" + 0.005*\"zuma\" + 0.004*\"trump\" + 0.003*\"senat\" + 0.003*\"suprem\" + 0.003*\"republican\" + 0.003*\"vote\" + 0.003*\"parti\" + 0.003*\"justic\"\n",
      "Topic: 8 Word: 0.005*\"parti\" + 0.003*\"catalan\" + 0.003*\"britain\" + 0.003*\"trump\" + 0.003*\"parliament\" + 0.003*\"european\" + 0.003*\"minist\" + 0.003*\"percent\" + 0.003*\"elect\" + 0.003*\"china\"\n",
      "Topic: 9 Word: 0.006*\"senat\" + 0.006*\"republican\" + 0.005*\"insur\" + 0.005*\"obamacar\" + 0.004*\"hous\" + 0.004*\"puerto\" + 0.004*\"trump\" + 0.004*\"legisl\" + 0.004*\"rico\" + 0.004*\"repeal\"\n",
      "Topic: 10 Word: 0.004*\"polic\" + 0.003*\"attack\" + 0.003*\"trump\" + 0.002*\"kill\" + 0.002*\"north\" + 0.002*\"peopl\" + 0.002*\"court\" + 0.002*\"citi\" + 0.002*\"arrest\" + 0.002*\"iran\"\n",
      "Topic: 11 Word: 0.020*\"myanmar\" + 0.016*\"rohingya\" + 0.010*\"bangladesh\" + 0.007*\"rakhin\" + 0.006*\"refuge\" + 0.005*\"muslim\" + 0.004*\"violenc\" + 0.004*\"flee\" + 0.004*\"ethnic\" + 0.003*\"buddhist\"\n",
      "Topic: 12 Word: 0.014*\"iraqi\" + 0.014*\"kurdish\" + 0.013*\"iraq\" + 0.008*\"baghdad\" + 0.006*\"referendum\" + 0.006*\"kirkuk\" + 0.006*\"region\" + 0.006*\"kurdistan\" + 0.006*\"islam\" + 0.006*\"kurd\"\n",
      "Topic: 13 Word: 0.005*\"fein\" + 0.005*\"sinn\" + 0.004*\"trump\" + 0.003*\"romania\" + 0.002*\"hous\" + 0.002*\"republican\" + 0.002*\"observatori\" + 0.002*\"white\" + 0.002*\"petersburg\" + 0.002*\"senat\"\n",
      "Topic: 14 Word: 0.015*\"korea\" + 0.013*\"north\" + 0.008*\"nuclear\" + 0.007*\"korean\" + 0.007*\"missil\" + 0.006*\"china\" + 0.005*\"south\" + 0.005*\"test\" + 0.005*\"japan\" + 0.004*\"pyongyang\"\n",
      "Topic: 15 Word: 0.015*\"turkey\" + 0.013*\"syrian\" + 0.011*\"syria\" + 0.010*\"turkish\" + 0.009*\"erdogan\" + 0.008*\"ankara\" + 0.007*\"islam\" + 0.006*\"deir\" + 0.006*\"russian\" + 0.006*\"raqqa\"\n",
      "Topic: 16 Word: 0.006*\"cambodia\" + 0.005*\"brexit\" + 0.005*\"sokha\" + 0.004*\"sisi\" + 0.004*\"davi\" + 0.004*\"trump\" + 0.004*\"britain\" + 0.004*\"egyptian\" + 0.003*\"egypt\" + 0.003*\"orban\"\n",
      "Topic: 17 Word: 0.008*\"russian\" + 0.007*\"moscow\" + 0.006*\"russia\" + 0.005*\"trump\" + 0.005*\"kremlin\" + 0.005*\"putin\" + 0.005*\"peskov\" + 0.004*\"ukrain\" + 0.004*\"congo\" + 0.003*\"elect\"\n",
      "Topic: 18 Word: 0.006*\"clinton\" + 0.005*\"trump\" + 0.004*\"investig\" + 0.004*\"republican\" + 0.004*\"senat\" + 0.003*\"democrat\" + 0.003*\"committe\" + 0.003*\"email\" + 0.003*\"hous\" + 0.003*\"campaign\"\n",
      "Topic: 19 Word: 0.008*\"hong\" + 0.008*\"kong\" + 0.006*\"trump\" + 0.004*\"republican\" + 0.004*\"turnbul\" + 0.004*\"clinton\" + 0.004*\"marriag\" + 0.004*\"christi\" + 0.004*\"senat\" + 0.003*\"democrat\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bb2d3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moscow',\n",
       " 'reuter',\n",
       " 'russia',\n",
       " 'say',\n",
       " 'hop',\n",
       " 'forthcom',\n",
       " 'talk',\n",
       " 'moscow',\n",
       " 'secretari',\n",
       " 'state',\n",
       " 'tillerson',\n",
       " 'product',\n",
       " 'say',\n",
       " 'extrem',\n",
       " 'worri',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'decid',\n",
       " 'unilater',\n",
       " 'attack',\n",
       " 'north',\n",
       " 'korea',\n",
       " 'north',\n",
       " 'korea',\n",
       " 'emerg',\n",
       " 'press',\n",
       " 'foreign',\n",
       " 'polici',\n",
       " 'problem',\n",
       " 'presid',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'pyongyang',\n",
       " 'conduct',\n",
       " 'nuclear',\n",
       " 'test',\n",
       " 'work',\n",
       " 'develop',\n",
       " 'nuclear',\n",
       " 'tip',\n",
       " 'missil',\n",
       " 'reach',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'offici',\n",
       " 'tell',\n",
       " 'reuter',\n",
       " 'weekend',\n",
       " 'navi',\n",
       " 'strike',\n",
       " 'group',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'aircraft',\n",
       " 'carrier',\n",
       " 'steam',\n",
       " 'western',\n",
       " 'pacif',\n",
       " 'forc',\n",
       " 'tillerson',\n",
       " 'visit',\n",
       " 'moscow',\n",
       " 'secretari',\n",
       " 'state',\n",
       " 'hold',\n",
       " 'talk',\n",
       " 'sergei',\n",
       " 'lavrov',\n",
       " 'russian',\n",
       " 'counterpart',\n",
       " 'wednesday',\n",
       " 'time',\n",
       " 'russia',\n",
       " 'tie',\n",
       " 'languish',\n",
       " 'post',\n",
       " 'cold',\n",
       " 'russian',\n",
       " 'foreign',\n",
       " 'ministri',\n",
       " 'statement',\n",
       " 'ahead',\n",
       " 'visit',\n",
       " 'say',\n",
       " 'concern',\n",
       " 'aspect',\n",
       " 'foreign',\n",
       " 'polici',\n",
       " 'includ',\n",
       " 'libya',\n",
       " 'yemen',\n",
       " 'syria',\n",
       " 'say',\n",
       " 'particular',\n",
       " 'concern',\n",
       " 'north',\n",
       " 'korea',\n",
       " 'worri',\n",
       " 'washington',\n",
       " 'mind',\n",
       " 'north',\n",
       " 'korea',\n",
       " 'hint',\n",
       " 'possibl',\n",
       " 'unilater',\n",
       " 'militari',\n",
       " 'scenario',\n",
       " 'statement',\n",
       " 'say',\n",
       " 'import',\n",
       " 'understand',\n",
       " 'talli',\n",
       " 'collect',\n",
       " 'oblig',\n",
       " 'nuclearis',\n",
       " 'korean',\n",
       " 'peninsula',\n",
       " 'underpin',\n",
       " 'secur',\n",
       " 'council',\n",
       " 'resolut',\n",
       " 'russia',\n",
       " 'slam',\n",
       " 'cruis',\n",
       " 'missil',\n",
       " 'strike',\n",
       " 'syrian',\n",
       " 'base',\n",
       " 'friday',\n",
       " 'call',\n",
       " 'illeg',\n",
       " 'attack',\n",
       " 'sovereign',\n",
       " 'state',\n",
       " 'grow',\n",
       " 'fear',\n",
       " 'moscow',\n",
       " 'trump',\n",
       " 'think',\n",
       " 'isolationist',\n",
       " 'increas',\n",
       " 'confid',\n",
       " 'forc',\n",
       " 'achiev',\n",
       " 'foreign',\n",
       " 'polici',\n",
       " 'goal',\n",
       " 'russian',\n",
       " 'foreign',\n",
       " 'ministri',\n",
       " 'say',\n",
       " 'tuesday',\n",
       " 'hop',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'agre',\n",
       " 'intern',\n",
       " 'investig',\n",
       " 'nerv',\n",
       " 'attack',\n",
       " 'prompt',\n",
       " 'missil',\n",
       " 'strike',\n",
       " 'syria',\n",
       " 'offici',\n",
       " 'afghanistan',\n",
       " 'talk',\n",
       " 'moscow',\n",
       " 'later',\n",
       " 'week']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c394e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.3875274658203125\t \n",
      "Topic: 0.026*\"north\" + 0.023*\"korea\" + 0.012*\"nuclear\" + 0.009*\"unit\" + 0.008*\"missil\" + 0.008*\"china\" + 0.008*\"sanction\" + 0.007*\"korean\" + 0.007*\"govern\" + 0.006*\"test\"\n",
      "\n",
      "Score: 0.37800100445747375\t \n",
      "Topic: 0.043*\"trump\" + 0.009*\"russian\" + 0.008*\"russia\" + 0.007*\"republican\" + 0.007*\"washington\" + 0.007*\"donald\" + 0.007*\"hous\" + 0.007*\"white\" + 0.006*\"offici\" + 0.006*\"putin\"\n",
      "\n",
      "Score: 0.22995060682296753\t \n",
      "Topic: 0.009*\"govern\" + 0.009*\"syria\" + 0.008*\"iran\" + 0.008*\"syrian\" + 0.008*\"forc\" + 0.008*\"islam\" + 0.007*\"unit\" + 0.007*\"saudi\" + 0.006*\"group\" + 0.005*\"iraq\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c07bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7459374070167542\t \n",
      "Topic: 0.015*\"korea\" + 0.013*\"north\" + 0.008*\"nuclear\" + 0.007*\"korean\" + 0.007*\"missil\" + 0.006*\"china\" + 0.005*\"south\" + 0.005*\"test\" + 0.005*\"japan\" + 0.004*\"pyongyang\"\n",
      "\n",
      "Score: 0.14362655580043793\t \n",
      "Topic: 0.005*\"parti\" + 0.003*\"catalan\" + 0.003*\"britain\" + 0.003*\"trump\" + 0.003*\"parliament\" + 0.003*\"european\" + 0.003*\"minist\" + 0.003*\"percent\" + 0.003*\"elect\" + 0.003*\"china\"\n",
      "\n",
      "Score: 0.10494652390480042\t \n",
      "Topic: 0.015*\"turkey\" + 0.013*\"syrian\" + 0.011*\"syria\" + 0.010*\"turkish\" + 0.009*\"erdogan\" + 0.008*\"ankara\" + 0.007*\"islam\" + 0.006*\"deir\" + 0.006*\"russian\" + 0.006*\"raqqa\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0ae8031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012*\"minist\" + 0.009*\"european\" + 0.008*\"govern\" + 0.006*\"britain\" + 0.006*\"deal\" + 0.006*\"trump\" + 0.006*\"countri\" + 0.006*\"year\" + 0.005*\"tell\" + 0.005*\"leader\"\n"
     ]
    }
   ],
   "source": [
    "heading = 'Agreement between President and basil Rajapaksha'\n",
    "topic_list_head = ''\n",
    "def lda_model_head (heading):\n",
    "    bow_vector = dictionary.doc2bow(preprocess(heading))\n",
    "    count=0\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        if count==0:\n",
    "            topic_list_head=lda_model.print_topic(index, 10)\n",
    "            print(topic_list_head)\n",
    "            count=count+1\n",
    "#         print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    ret_head = topic_list_head\n",
    "    return ret_head\n",
    "top_head = lda_model_head(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "321fd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is whee the model will be called by user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84ba99e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012*\"minist\" + 0.009*\"european\" + 0.008*\"govern\" + 0.006*\"britain\" + 0.006*\"deal\" + 0.006*\"trump\" + 0.006*\"countri\" + 0.006*\"year\" + 0.005*\"tell\" + 0.005*\"leader\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "print(top_head)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6cf89027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minist', 'european', 'govern', 'britain', 'deal', 'trump', 'countri', 'year', 'tell', 'leader']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def best_head (top_head):\n",
    "    rest = top_head.translate(remove_digits)\n",
    "    final_list_head = re.sub(r'[^\\w\\s]', '', rest)\n",
    "    flh = final_list_head.split()\n",
    "    return flh\n",
    "\n",
    "best_head_final = best_head(top_head)\n",
    "print(best_head1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8cb0585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9399711489677429\t Topic: 0.008*\"year\" + 0.008*\"percent\" + 0.008*\"trump\" + 0.006*\"peopl\" + 0.006*\"govern\" + 0.006*\"hurrican\" + 0.006*\"million\" + 0.005*\"irma\" + 0.005*\"florida\" + 0.005*\"island\"\n"
     ]
    }
   ],
   "source": [
    "content = 'Close on the heels of the project that translocated cheetahs from Namibia, the Indian government is considering a proposal from Colombo to export a number of gaurs, or Indian bisons, to Sri Lanka to revive the population of gavaras that have been extinct in the island since the end of the 17th century.'\n",
    "topic_list_content = ''\n",
    "def lad_model_content(content):\n",
    "    bow_vector = dictionary.doc2bow(preprocess(content))\n",
    "    count=0\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        if count==0:\n",
    "            topic_list_content=lda_model.print_topic(index, 10)\n",
    "            count=count+1\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    ret_content = topic_list_content\n",
    "    return ret_content\n",
    "top_content = lad_model_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c213086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'percent', 'trump', 'peopl', 'govern', 'hurrican', 'million', 'irma', 'florida', 'island']\n"
     ]
    }
   ],
   "source": [
    "def best_content(top_content):\n",
    "    rest = top_content.translate(remove_digits)\n",
    "    final_list_content = re.sub(r'[^\\w\\s]', '', rest)\n",
    "    print(final_list_content.split())\n",
    "    flc = (final_list_content.split())\n",
    "# flc = convert(final_list_content.split())\n",
    "    return flc\n",
    "best_content_final = best_content(top_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03479882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine will be taken from here for both heading and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8d526a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "l1 =[];l2 =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe0fb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minist', 'leader', 'european', 'tell', 'countri', 'britain', 'year', 'deal', 'govern', 'trump'}\n",
      "{'peopl', 'island', 'hurrican', 'percent', 'florida', 'million', 'irma', 'year', 'govern', 'trump'}\n"
     ]
    }
   ],
   "source": [
    "x_set = {w for w in best_head_final if not w in sw}\n",
    "y_set = {w for w in best_content_final if not w in sw}\n",
    "print(x_set)\n",
    "print (y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9c8cdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvector = x_set.union(y_set) \n",
    "for w in rvector:\n",
    "    if w in best_head_final: l1.append(1) # create a vector\n",
    "    else: l1.append(0)\n",
    "    if w in best_content_final: l2.append(1)\n",
    "    else: l2.append(0)\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b3fe8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity:  0.3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rvector)):\n",
    "        c+= l1[i]*l2[i]\n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "print(\"similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52076dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hwo to get the accuracy of the LDA model\n",
    "#  train the dataset inlucding sri lanka news\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1970fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model using the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3874c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(top_content, open('LdaModel', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "48ec94dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'top_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m test_10 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLdaModel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest_10\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_content\u001b[49m(content)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'top_content'"
     ]
    }
   ],
   "source": [
    "test_10 = pickle.load(open('LdaModel', 'rb'))\n",
    "test_10.top_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994f404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "67c0f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"model\": top_content}\n",
    "with open ('saved_steps.pkl' , 'wb') as file:\n",
    "    pickle.dump(data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d9281a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('saved_steps.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "load_model = data[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5fa24c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'top_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [131]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_content\u001b[49m(content)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'top_content'"
     ]
    }
   ],
   "source": [
    "y_pred = load_model.top_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f440d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
